# Some good combinations for HW1 problem1
* Batch size and learning rate to choose
|Batch size|eta(learning rate)|
|----------|------------------|
|8|1e-5|
|4|1e-5|
|50|1e-5|
|25|1e-06|
|40|1e-06|


* 所以整體流程
```
我先構件出一個 w1 w2 w3..等等 隨機的
然後我在用 SGD找優話，過程中因為 backpropogation 從 w3 逆推回去就會用連鎖律帶出w3 2 1的關係
最後 隨著SGD的優話就會讓錯誤率下降
但是要下降到什麼程度，什麼時候該停，下降的好不好，就是要看 
會影響 SGD 進行的參數，這樣
```

* 0411 問助教的疑問
* 第一，是否需要固定神經網路中的 random seed --> 想法是 因為每一次神經網路在初始化的時候，我是用一個常態分佈的隨機變數，來做初始化，之後再依據梯度下降的算法更新優化整體神經網路
* 第二 即便我固定了神經網路中的 random seed，我期望是random shuffle的順序會一致，因此每一次做batch 的時候才能設為控制變因，而操縱變因就是 learning rate 和 batch size 了（但我發現即便是固定所有東西，同一個神經網路跑兩次 學習曲線也會有不同結果，不知道是不是初始化的 np.randn() 問題)
* 承上題，那如果是這樣的話 ，我覺得random seed 就沒有必要設定，因為我們探討的是在經過梯度下降後能否降低損失函數，如果固定隨機的種子，那他下降可能就只是一種特例（我當初設定隨機種子要一樣，實際上是為了比較norm vs no-norm 的差別）
